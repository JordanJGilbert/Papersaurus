import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from fastmcp import FastMCP
import uuid
import re
import json
import requests
import base64
import filetype
from PIL import Image
from io import BytesIO
import asyncio
import hashlib
from typing import List, Optional, Dict, Tuple, Any
from utils.constants import DOMAIN
import vertexai
from vertexai.preview.vision_models import ImageGenerationModel
from PIL import Image as PILImage
import time
# Added for gpt-image-1 support
from openai import OpenAI
# import base64 # Already imported globally

mcp = FastMCP("Image, Video, Music, and Speech Services Server")

print("Image, Video, Music, and Speech Services Server ready - Vertex AI projects will be initialized on first use")

@mcp.tool(
    annotations={
        "outputSchema": {
            "type": "object",
            "properties": {
                "status": {
                    "type": "string",
                    "enum": ["success", "error"],
                    "description": "Overall analysis status"
                },
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "status": {
                                "type": "string",
                                "enum": ["success", "error"],
                                "description": "Individual image analysis status"
                            },
                            "analysis": {
                                "type": "string",
                                "description": "Detailed analysis text from Gemini"
                            },
                            "url": {
                                "type": "string",
                                "format": "uri",
                                "description": "Original image URL that was analyzed"
                            },
                            "message": {
                                "type": "string",
                                "description": "Error message if analysis failed"
                            }
                        },
                        "required": ["status", "url"],
                        "description": "Analysis result for individual image"
                    },
                    "description": "Array of analysis results, one per image"
                }
            },
            "required": ["status", "results"]
        }
    }
)
async def analyze_images(urls: list, analysis_prompt: str = "Describe this image in detail.") -> dict:
    """
    Analyzes a list of images by downloading each from its URL and using Gemini to interpret its contents.
    Args:
        urls (list): List of image URLs to analyze.
        analysis_prompt (str): Instructions for how Gemini should analyze the images.
    Returns:
        dict: {"status": "success", "results": [ ... ]} or {"status": "error", "message": ...}
    """
    from llm_adapters import get_llm_adapter, StandardizedMessage, StandardizedLLMConfig, AttachmentPart

    async def analyze_single_image(url):
        try:
            response = await asyncio.to_thread(requests.get, url, timeout=10)
            response.raise_for_status()
            content_type = response.headers.get('Content-Type', '')
            if not content_type.startswith('image/'):
                return {"status": "error", "message": f"URL does not point to an image (content type: {content_type})", "url": url}
            image_bytes_io = BytesIO(response.content)
            image = await asyncio.to_thread(Image.open, image_bytes_io)
            img_bytes_for_gemini = BytesIO()
            img_format = image.format if image.format and image.format.upper() in Image.SAVE.keys() else "PNG"
            await asyncio.to_thread(image.save, img_bytes_for_gemini, format=img_format)
            img_bytes_for_gemini.seek(0)

            # Use the adapter for Gemini LLM call
            adapter = get_llm_adapter("gemini-2.5-flash-preview-05-20")  # Will be forced to Flash in the adapter
            attachment = AttachmentPart(mime_type=content_type, data=img_bytes_for_gemini.getvalue(), name=url)
            history = [
                StandardizedMessage(
                    role="user",
                    content=analysis_prompt,
                    attachments=[attachment]
                )
            ]
            llm_config = StandardizedLLMConfig()
            llm_response = await adapter.generate_content(
                model_name="gemini-2.5-flash-preview-05-20",
                history=history,
                tools=None,
                config=llm_config
            )
            analysis = llm_response.text_content
            if llm_response.error:
                return {"status": "error", "message": f"LLM error: {llm_response.error}", "url": url}
            return {"status": "success", "analysis": analysis, "url": url}
        except requests.exceptions.RequestException as e:
            return {"status": "error", "message": f"Error downloading image: {str(e)}", "url": url}
        except Exception as e:
            return {"status": "error", "message": f"Error analyzing image: {str(e)}", "url": url}

    if not isinstance(urls, list):
        return {"status": "error", "message": "urls must be a list of image URLs."}
    tasks = [analyze_single_image(url) for url in urls]
    results = await asyncio.gather(*tasks)
    return {"status": "success", "results": results}

def download_and_encode_image(url):
    """
    Downloads an image from the given URL and returns (base64_data, media_type).
    Returns (None, None) if download or type detection fails.
    """
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        content_type = resp.headers.get('Content-Type', '')
        if not content_type.startswith('image/'):
            return None, None
        b64_data = base64.b64encode(resp.content).decode('utf-8')
        return b64_data, content_type
    except Exception:
        return None, None

def sanitize_for_path(name_part: str) -> str:
    """Sanitizes a string to be safe for use as part of a directory or file name."""
    if not isinstance(name_part, str):
        name_part = str(name_part)
    # Remove or replace potentially problematic characters
    name_part = name_part.replace('+', '') # Common in user_numbers
    # Hash group IDs for consistent, short, safe names
    if name_part.startswith('group_'):
        group_id_val = name_part[len('group_'):]
        hash_val = hashlib.md5(group_id_val.encode()).hexdigest()[:12] # Slightly longer hash
        name_part = f"group_{hash_val}"
    
    name_part = re.sub(r'[^\w.-]', '_', name_part) # Allow word chars, dots, hyphens; replace others with underscore
    name_part = re.sub(r'_+', '_', name_part) # Collapse multiple underscores
    name_part = name_part.strip('_.- ') # Strip leading/trailing problematic chars
    if not name_part: # Handle empty string after sanitization
        return f"sanitized_{uuid.uuid4().hex[:8]}"
    return name_part

async def _generate_images_with_prompts_concurrent(user_number, prompts, model_version="imagen-4.0-generate-preview-06-06", input_images=None, aspect_ratio="16:9"):
    import re
    import hashlib
    import time
    # Ensure base64 is available for OpenAI path; it's already globally imported but good to note.
    # import base64 
    from PIL import Image # Ensure PIL.Image is available
    from io import BytesIO # Ensure BytesIO is available
    import uuid # Ensure uuid is available
    import os # Ensure os is available
    from utils.constants import DOMAIN # Ensure DOMAIN is available
    
    # Use default user number if empty
    if not user_number or user_number == "--user_number_not_needed--":
        user_number = "+17145986105"
    
    def sanitize_for_path_local(s_input): # Renamed to avoid conflict with global sanitize_for_path
        s = str(s_input).replace('+', '')
        if s.startswith('group_'):
            group_id = s[len('group_'):]
            hash_val = hashlib.md5(group_id.encode()).hexdigest()[:8]
            s = f"group_{hash_val}"
        s = re.sub(r'[^a-zA-Z0-9]', '_', s)
        s = re.sub(r'_+', '_', s)
        s = s.strip('_')
        if not s: # Handle empty string after sanitization
            return f"sanitized_{uuid.uuid4().hex[:8]}"
        return s

    user_number_safe = sanitize_for_path_local(user_number)
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../user_data'))
    image_dir = os.path.join(base_dir, user_number_safe, "images")
    os.makedirs(image_dir, exist_ok=True)

    if model_version == "gpt-image-1":
        # OpenAI GPT Image Logic
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        if not openai_api_key:
            print("Error: OPENAI_API_KEY environment variable not set.")
            return {"status": "error", "message": "OPENAI_API_KEY environment variable not set for gpt-image-1 model."}
        
        try:
            # The OpenAI client will use the OPENAI_API_KEY env var by default
            client = OpenAI() 
        except Exception as e:
            print(f"Error initializing OpenAI client: {str(e)}")
            return {"status": "error", "message": f"Failed to initialize OpenAI client: {str(e)}"}

        all_prompt_results = []
        any_errors_openai = False
        
        async def generate_for_single_prompt_openai(prompt_text, prompt_idx_openai):
            try:
                print(f"🎯 OpenAI Prompt #{prompt_idx_openai}: '{prompt_text[:50]}...' using model {model_version}")
                start_time_openai = time.time()
                
                # Prepare API parameters
                api_params = {
                    "model": "gpt-image-1",
                    "prompt": prompt_text,
                    "size": "1536x864", # Standard 16:9 aspect ratio
                    "n": 1, # Generate one image per prompt
                    # Note: gpt-image-1 automatically returns b64_json format, no response_format parameter needed
                }
                
                # Add input image if provided (for image context/reference)
                if input_images:
                    # Use the corresponding input image for this prompt, or the first one if only one provided
                    if prompt_idx_openai < len(input_images):
                        input_image_url = input_images[prompt_idx_openai]
                    else:
                        input_image_url = input_images[0]  # Use first image for all prompts
                    
                    print(f"🖼️ Using input image as context: {input_image_url[:50]}...")
                    
                    # Download and prepare the input image
                    try:
                        if input_image_url.startswith("http://") or input_image_url.startswith("https://"):
                            resp = await asyncio.to_thread(requests.get, input_image_url, timeout=20)
                            resp.raise_for_status()
                            img_bytes = resp.content
                        elif len(input_image_url) > 100 and input_image_url.endswith(('=', '==')):
                            img_bytes = base64.b64decode(input_image_url)
                        else:
                            print(f"⚠️ Invalid input image format for prompt #{prompt_idx_openai}, skipping image context")
                            img_bytes = None
                        
                        if img_bytes:
                            # Validate and prepare image file
                            img_pil = await asyncio.to_thread(Image.open, BytesIO(img_bytes))
                            
                            # Convert to supported format if needed
                            if img_pil.format not in ['PNG', 'WEBP', 'JPEG']:
                                # Convert to PNG
                                img_buffer = BytesIO()
                                await asyncio.to_thread(img_pil.save, img_buffer, format='PNG')
                                img_bytes = img_buffer.getvalue()
                                img_format = 'PNG'
                            else:
                                img_format = img_pil.format
                            
                            # Create file-like object for API
                            if img_format == 'JPEG':
                                image_file = BytesIO(img_bytes)
                                image_file.name = "reference.jpg"
                            elif img_format == 'PNG':
                                image_file = BytesIO(img_bytes)
                                image_file.name = "reference.png"
                            elif img_format == 'WEBP':
                                image_file = BytesIO(img_bytes)
                                image_file.name = "reference.webp"
                            
                            # Add image to API parameters
                            api_params["image"] = image_file
                            print(f"✅ Added input image context: {img_format}, {len(img_bytes)} bytes")
                            
                    except Exception as e:
                        print(f"⚠️ Failed to process input image for prompt #{prompt_idx_openai}: {e}")
                        # Continue without image context
                
                                # Make API call - use edit endpoint if image provided, generate endpoint otherwise
                if "image" in api_params:
                    # Use edit endpoint for image-to-image generation
                    image_file = api_params.pop("image")
                    response_openai = await asyncio.to_thread(
                        client.images.edit,
                        image=image_file,
                        **api_params
                    )
                else:
                    # Use generate endpoint for text-to-image generation
                    response_openai = await asyncio.to_thread(
                        client.images.generate,
                        **api_params
                    )
                
                end_time_openai = time.time()
                duration_openai = end_time_openai - start_time_openai
                print(f"✅ OpenAI API CALL COMPLETE - Prompt #{prompt_idx_openai} (took {duration_openai:.2f}s)")

                if not response_openai.data or not response_openai.data[0].b64_json:
                    print(f"Error: OpenAI API did not return image data for prompt: {prompt_text}")
                    return {"error": f"OpenAI API did not return image data for prompt: {prompt_text}"}

                image_base64_data = response_openai.data[0].b64_json
                image_bytes_data = base64.b64decode(image_base64_data)
                
                pil_image_obj = Image.open(BytesIO(image_bytes_data))
                
                # Save image
                # Filename uses a prefix and a portion of UUID.
                filename_openai = f"gpt_img_{uuid.uuid4().hex[:8]}.png" # Save as PNG
                local_path_openai = os.path.join(image_dir, filename_openai)
                # os.makedirs(image_dir, exist_ok=True) # Already done above

                await asyncio.to_thread(pil_image_obj.save, local_path_openai, format="PNG")
                
                image_url_openai = f"{DOMAIN}/user_data/{user_number_safe}/images/{filename_openai}"
                print(f"✅ OpenAI Success: Prompt #{prompt_idx_openai}. Generated image: {image_url_openai}")
                return [image_url_openai] # List containing the URL

            except Exception as e_openai:
                error_str_openai = str(e_openai)
                print(f"❌ OpenAI Error: Prompt #{prompt_idx_openai} ('{prompt_text[:30]}...'): {error_str_openai}")
                return {"error": f"OpenAI image generation failed for prompt '{prompt_text}': {error_str_openai}"}

        openai_tasks = [generate_for_single_prompt_openai(p, i) for i, p in enumerate(prompts)]
        
        # Debug: Show batch start time for OpenAI
        batch_start_time_openai = time.time()
        batch_start_str_openai = time.strftime("%H:%M:%S", time.localtime(batch_start_time_openai))
        print(f"\n⏱️ OpenAI BATCH START [{batch_start_str_openai}] - Launching {len(prompts)} concurrent API calls...")

        results_from_openai_api = await asyncio.gather(*openai_tasks, return_exceptions=False) # Errors handled within helper

        # Debug: Show batch completion time for OpenAI
        batch_end_time_openai = time.time()
        batch_duration_openai = batch_end_time_openai - batch_start_time_openai
        batch_end_str_openai = time.strftime("%H:%M:%S", time.localtime(batch_end_time_openai))
        print(f"\n🏁 OpenAI BATCH COMPLETE [{batch_end_str_openai}] - All {len(prompts)} requests finished (total time: {batch_duration_openai:.2f}s)")


        for i_openai, r_openai in enumerate(results_from_openai_api):
            if isinstance(r_openai, dict) and "error" in r_openai:
                any_errors_openai = True
                all_prompt_results.append(r_openai) 
            elif isinstance(r_openai, list):
                all_prompt_results.append(r_openai)
            else:
                any_errors_openai = True
                error_msg_detail = f"Unexpected result type for OpenAI prompt index {i_openai}: {type(r_openai)}"
                print(error_msg_detail)
                all_prompt_results.append({"error": error_msg_detail})
        
        if any_errors_openai:
            # Check if all failed
            if all(isinstance(res, dict) and "error" in res for res in all_prompt_results):
                 return {"status": "error", "message": "All image generations failed with OpenAI. Check results for details.", "results": all_prompt_results}
            return {"status": "partial_error", "message": "Some images failed to generate with OpenAI. Check results for details.", "results": all_prompt_results}
        
        return {"status": "success", "results": all_prompt_results}

    else:
        # --- Existing Vertex AI Logic ---
        # --- Multiple Vertex AI Project Configuration ---
        vertex_projects = [
            "gen-lang-client-0761146080",
            "maximal-totem-456502-h3",
            "imagen-api-3", 
            "imagen-api-4",
            "imagen-api-5",
            "imagen-api-6-461900",
            "imagen-api-5-461900",
            "imagen-api-8",
            "imagen-api-9",
            "imagen-api-10",
            "imagen-api-11",
            "imagen-api-12",
        ]
        location = "us-central1"
        
        # Create a lock for vertex AI initialization to prevent race conditions
        vertex_init_lock = asyncio.Lock()
        
        # Initialize models for each project
        generation_models = []
        for i, project_id in enumerate(vertex_projects):
            try:
                # Re-initialize Vertex AI for each project
                vertexai.init(project=project_id, location=location)
                model = ImageGenerationModel.from_pretrained(model_version)
                generation_models.append((project_id, model))
                print(f"✅ Initialized Vertex AI project {i+1}/{len(vertex_projects)}: {project_id}")
            except Exception as e:
                print(f"❌ Failed to initialize Vertex AI project {project_id}: {e}")
                continue
        
        if not generation_models:
            print("Error: No Vertex AI projects could be initialized.")
            return {"status": "error", "message": "No Vertex AI projects configured successfully."}

        num_projects = len(generation_models)
        print(f"🚀 Initialized image generation with {num_projects} Vertex AI projects for rate limit distribution.")

        # --- Helper Functions ---
        async def process_image_response(response, prompt, prompt_idx):
            """Processes images from a prompt response concurrently."""
            os.makedirs(image_dir, exist_ok=True)

            async def _save_and_process_single_image(generated_image, index):
                """Handles saving the PNG image and returning its URL."""
                try:
                    # For Vertex AI ImageGenerationModel, use the _pil_image attribute
                    if hasattr(generated_image, '_pil_image'):
                        image = generated_image._pil_image
                    else:
                        print(f"Could not extract PIL image from generated_image object: {type(generated_image)}, dir: {dir(generated_image)}")
                        return None
                    
                    # Filename now uses only a prefix and a portion of UUID.
                    # Using 8 hex characters from UUID for good collision resistance.
                    filename = f"img_{uuid.uuid4().hex[:8]}.png"
                    local_path = os.path.join(image_dir, filename)

                    # Save the original PNG image to disk non-blockingly
                    await asyncio.to_thread(image.save, local_path)

                    # Construct the URL using the original PNG filename saved to disk
                    image_url = f"{DOMAIN}/user_data/{user_number_safe}/images/{filename}"
                    return image_url
                except Exception as e:
                    print(f"Error processing image {index} for prompt {prompt_idx} ('{prompt[:30]}...'): {e}")
                    return None # Return None on error for this specific image

            # Create and run tasks concurrently for all images from this single prompt response
            # For Vertex AI, response.images contains the list of GeneratedImage objects
            if hasattr(response, 'images'):
                images_list = response.images
            else:
                print(f"Unexpected response format from Vertex AI: {type(response)}, expected ImageGenerationResponse with 'images' attribute")
                return []

            save_tasks = [
                _save_and_process_single_image(img, i)
                for i, img in enumerate(images_list)
            ]
            
            image_url_results = await asyncio.gather(*save_tasks)

            # Filter out None results (errors during saving/processing)
            successful_urls = [url for url in image_url_results if url is not None]
            
            # Return only the successfully processed URLs
            return successful_urls

        async def generate_for_prompt(prompt, idx, project_model_tuple):
            project_id, generation_model = project_model_tuple
            project_index = generation_models.index(project_model_tuple)

            try:
                print(f"🎯 Prompt #{idx}: '{prompt[:50]}...' → Project: {project_id} (#{project_index + 1}/{num_projects})")
                
                # Use lock to prevent race conditions during vertex AI initialization
                async with vertex_init_lock:
                    # Re-initialize Vertex AI for this specific project before generating
                    vertexai.init(project=project_id, location=location)
                
                # Debug: Show API call start time
                start_time = time.time()
                current_time = time.strftime("%H:%M:%S", time.localtime(start_time))
                print(f"🚀 API CALL START [{current_time}] - Prompt #{idx} → {project_id}")
                
                response = await asyncio.to_thread(
                    generation_model.generate_images,
                    prompt=prompt,
                    number_of_images=1,
                    aspect_ratio=aspect_ratio, # Use the passed aspect_ratio
                    negative_prompt="",
                    person_generation="", # Keeping other defaults
                    safety_filter_level="",
                    add_watermark=True
                )
                
                # Debug: Show API call completion time
                end_time = time.time()
                duration = end_time - start_time
                completion_time = time.strftime("%H:%M:%S", time.localtime(end_time))
                print(f"✅ API CALL COMPLETE [{completion_time}] - Prompt #{idx} → {project_id} (took {duration:.2f}s)")
                
                urls = await process_image_response(response, prompt, idx)
                
                if not urls:
                    print(f"⚠️ Warning: Vertex AI project {project_id} generated response for prompt '{prompt}' (idx: {idx}), but processing/saving failed for all images.")
                    return {"error": f"Image generation API succeeded for prompt '{prompt}' with project {project_id}, but processing/saving failed for all images."}
                
                print(f"✅ Success: Prompt #{idx} with {project_id}. Generated {len(urls)} URLs.")
                return urls

            except Exception as e:
                error_str = str(e)
                print(f"❌ Error: Prompt #{idx} with {project_id}: {error_str}")
                
                # Check if it's a rate limit/quota error for logging purposes
                is_rate_limit = False
                try:
                    from google.api_core import exceptions as google_exceptions
                    if isinstance(e, google_exceptions.ResourceExhausted): is_rate_limit = True
                    elif isinstance(e, google_exceptions.GoogleAPIError) and e.code == 429: is_rate_limit = True
                except ImportError: pass
                if not is_rate_limit and hasattr(e, 'code') and e.code == 429: is_rate_limit = True
                elif not is_rate_limit and ("rate limit" in error_str.lower() or "quota" in error_str.lower() or "429" in error_str): is_rate_limit = True

                if is_rate_limit:
                    error_message = f"⏰ Rate limit hit on project {project_id} for prompt '{prompt}' - this is why we cycle projects!"
                    print(f"📊 Rate limit status: Project {project_id} ({project_index + 1}/{num_projects}) hit rate limit")
                else:
                    error_message = f"Image generation failed on project {project_id} for prompt '{prompt}': {error_str}"

                print(f"DEBUG: Failing prompt '{prompt}' due to error: {error_message}")
                return {"error": error_message}

        # --- Run Concurrent Tasks with Project Distribution ---
        print(f"📝 Distributing {len(prompts)} prompts across {num_projects} projects:")
        for i, prompt in enumerate(prompts):
            project_idx = i % num_projects
            project_id = generation_models[project_idx][0]
            print(f"   Prompt #{i}: {prompt[:30]}... → {project_id}")
        
        # Debug: Show batch start time
        batch_start_time = time.time()
        batch_start_str = time.strftime("%H:%M:%S", time.localtime(batch_start_time))
        print(f"\n⏱️ BATCH START [{batch_start_str}] - Launching {len(prompts)} concurrent API calls...")
        
        tasks = [generate_for_prompt(prompt, idx, generation_models[idx % num_projects]) for idx, prompt in enumerate(prompts)]
        results = await asyncio.gather(*tasks, return_exceptions=False) # Errors handled within generate_for_prompt

        # Debug: Show batch completion time
        batch_end_time = time.time()
        batch_duration = batch_end_time - batch_start_time
        batch_end_str = time.strftime("%H:%M:%S", time.localtime(batch_end_time))
        print(f"\n🏁 BATCH COMPLETE [{batch_end_str}] - All {len(prompts)} requests finished (total time: {batch_duration:.2f}s)")

        # --- Process Results ---
        final_results = []
        any_errors = False
        error_message = "Multiple errors occurred during image generation."

        for i, r in enumerate(results):
            if isinstance(r, dict) and "error" in r:
                any_errors = True
                # Use the specific error message from the result
                error_message = r["error"]
                print(f"Error reported for prompt index {i}: {r['error']}")
                final_results.append({"error": r["error"]}) # Keep error info per prompt
            elif isinstance(r, list):
                 final_results.append(r)
            else:
                any_errors = True
                error_message = f"Unexpected result type for prompt index {i}: {type(r)}"
                print(error_message)
                final_results.append({"error": "Unexpected result type."})

        if any_errors:
             # Return partial success with errors marked per prompt
             return {"status": "partial_error", "message": "Some images failed to generate. Check results for details.", "results": final_results}

        # If no errors found in results
        return {"status": "success", "results": final_results}

@mcp.tool(
    annotations={
        "outputSchema": {
            "type": "object",
            "properties": {
                "status": {
                    "type": "string",
                    "enum": ["success", "partial_error", "error"],
                    "description": "Overall generation status"
                },
                "message": {
                    "type": "string",
                    "description": "Human-readable status message"
                },
                "results": {
                    "type": "array",
                    "items": {
                        "oneOf": [
                            {
                                "type": "array",
                                "items": {"type": "string", "format": "uri"},
                                "description": "Array of generated image URLs"
                            },
                            {
                                "type": "object",
                                "properties": {
                                    "error": {"type": "string"}
                                },
                                "required": ["error"],
                                "description": "Error object for failed generations"
                            }
                        ]
                    },
                    "description": "Array of results, one per prompt"
                }
            },
            "required": ["status", "results"]
        }
    }
)
async def generate_images_with_prompts(
    user_number: str = "+17145986105",
    prompts: list = None,
    model_version: str = "imagen-4.0-generate-preview-06-06",
    input_images: list = None,
    aspect_ratio: str = "16:9" # Add aspect_ratio parameter
) -> dict:
    """
    Generate one image for each prompt using Google's Imagen 4.0 model or OpenAI's GPT-1.
    All images will be generated with the specified aspect ratio (defaults to 16:9).
    
    **NEW: Image Input Support for GPT-1**
    When using GPT-1 with input_images, the model can analyze reference images and use them as context
    for generating new images. This is perfect for style transfer, handwriting replication, and 
    reference-based generation.

    Args:
        user_number (str): The user's unique identifier (used for directory structure).
        prompts (list): List of text prompts. Each prompt will generate one image.
        model_version (str): Model version to use. Options:
                           - "imagen-4.0-generate-preview-06-06" (default, Vertex AI, balanced)
                           - "imagen-4.0-fast-generate-preview-06-06" (Vertex AI, faster generation)
                           - "imagen-4.0-ultra-generate-preview-06-06" (Vertex AI, highest quality)
                           - "gpt-image-1" (OpenAI, supports image inputs as context)
        input_images (list, optional): List of image URLs or base64 strings to use as context/reference.
                                     Only supported by gpt-image-1. If provided, should match the length 
                                     of prompts list, or provide one image to use for all prompts.
                                     Images are used as visual context for generation.
        aspect_ratio (str): Aspect ratio for generated images (e.g., "16:9", "1:1", "9:16"). 
                            Defaults to "16:9". GPT-1 will use its closest supported size.

    Returns:
        dict: {"status": "success", "results": [ [url], ... ]} or {"status": "error", "message": ...}

    **GPT-1 with Image Context Examples:**
    ```python
    # Use handwriting sample as reference for generating new message
    result = await generate_images_with_prompts(
        prompts=["Write 'Happy Birthday!' in the same handwriting style as this sample"],
        model_version="gpt-image-1",
        input_images=["https://example.com/handwriting_sample.jpg"]
    )

    # Style transfer from reference image
    result = await generate_images_with_prompts(
        prompts=["Create a landscape painting in the same artistic style as this reference"],
        model_version="gpt-image-1", 
        input_images=["https://example.com/art_reference.jpg"]
    )

    # Multiple prompts with same reference
    result = await generate_images_with_prompts(
        prompts=["Write 'Hello'", "Write 'Goodbye'"],
        model_version="gpt-image-1",
        input_images=["https://example.com/handwriting.jpg"]  # Same reference for both
    )
    ```

    **Image Input Requirements (GPT-1 only):**
    - Supported formats: PNG, JPEG, WebP, non-animated GIF
    - Maximum size: 50MB per image
    - Maximum: 500 images per request
    - Images are processed as visual context alongside text prompts
    - Can provide one image for all prompts, or one image per prompt
    """
    # Use default user number if empty
    if not user_number or user_number == "--user_number_not_needed--":
        user_number = "+17145986105"
    
    # Validate model version
    valid_models = [
        "imagen-4.0-generate-preview-06-06",
        "imagen-4.0-fast-generate-preview-06-06", 
        "imagen-4.0-ultra-generate-preview-06-06",
        "gpt-image-1"  # Added gpt-image-1
    ]
    
    if model_version not in valid_models:
        return {
            "status": "error",
            "message": f"Invalid model_version '{model_version}'. Must be one of: {', '.join(valid_models)}"
        }
    
    # Validate aspect_ratio for Vertex AI models (more flexible)
    # For GPT-1, it will use its fixed size which we've set to 16:9 (1536x864)
    # No specific validation needed here as Imagen supports various ratios like "1:1", "16:9", "9:16", "4:3", "3:4"
    
    return await _generate_images_with_prompts_concurrent(user_number, prompts, model_version, input_images, aspect_ratio)

@mcp.tool(
    annotations={
        "outputSchema": {
            "type": "object",
            "properties": {
                "status": {
                    "type": "string",
                    "enum": ["success", "partial_error", "error"],
                    "description": "Overall editing status"
                },
                "message": {
                    "type": "string",
                    "description": "Human-readable status message"
                },
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "status": {
                                "type": "string",
                                "enum": ["success", "error"],
                                "description": "Individual image editing status"
                            },
                            "original_image": {
                                "type": "string",
                                "description": "Input image identifier (URL or base64)"
                            },
                            "edited_url": {
                                "type": "string",
                                "format": "uri",
                                "description": "URL to the edited image (only present on success)"
                            },
                            "message": {
                                "type": "string",
                                "description": "Status message for this image"
                            }
                        },
                        "required": ["status", "original_image", "message"],
                        "description": "Editing result for individual image"
                    },
                    "description": "Array of editing results, one per image"
                }
            },
            "required": ["status", "message", "results"]
        }
    }
)
async def edit_images(
    images: List[str],
    edit_prompt: str,
    user_number: str = "+17145986105",
    model: str = "gpt-image-1",
    background: str = "auto",
    mask: str = None,
    output_format: str = "png",
    quality: str = "auto",
    output_compression: int = 100,
    size: str = "1024x1536",
    n: int = 1
) -> dict:
    """
    Edits images using either OpenAI's GPT-1 (default) or Google's Gemini image generation models.
    GPT-1 is now the default and recommended model for image editing.

    **🎨 PERFECT FOR STYLE TRANSFER & REFERENCE-BASED EDITING:**
    This tool excels at using input images as references to apply styling, context, or transformations.
    You can take any image and transform it to match a desired style, mood, or aesthetic.

    Args:
        images (List[str]): List of URLs or base64-encoded images to be edited.
                            **IMPORTANT**: If the user provided an image directly with their request (e.g., as an attachment in a chat),
                            its URL might be mentioned in the prompt context (e.g., "[Context: The following files were attached... Attachment 0 (Name: user_img.jpg, URL: https://...)]").
                           If so, you SHOULD use that specific URL for this 'images' argument when the edit request pertains to that user-provided image.
        edit_prompt (str): Text describing the desired edit (applied to all images).
                          Max length: 32,000 characters for gpt-image-1, 1,000 for dall-e-2.
                          **TIP**: Be specific about styles, moods, and transformations you want applied.
        user_number (str): User identifier for saving the results.
        model (str): Model to use for editing. Options:
                    - "gpt-image-1" (default, OpenAI, highest quality, supports up to 16 images)
                    - "gemini" (Google Gemini image generation model)
        background (str): Background transparency for gpt-image-1. Options: "transparent", "opaque", "auto" (default).
                         Only works with png/webp output formats. Ignored for other models.
        mask (str): Optional mask image URL for gpt-image-1. PNG with transparent areas indicating where to edit.
                   Must have same dimensions as input image. Ignored for other models.
        output_format (str): Output format for gpt-image-1. Options: "png" (default), "jpeg", "webp".
                            Ignored for other models.
        quality (str): Image quality for gpt-image-1. Options: "auto" (default), "high", "medium", "low".
                      Ignored for other models.
        output_compression (int): Compression level 0-100% for gpt-image-1 with webp/jpeg formats.
                                 Default: 100. Ignored for other models.
        size (str): Output size for gpt-image-1. Options: "auto" (default), "1024x1024", "1536x1024", "1024x1536", "1536x1536".
                   Ignored for other models.
        n (int): Number of edited images to generate per input image (1-10). Default: 1.
                Only supported by gpt-image-1.

    Returns:
        dict: {"status": "success", "results": [{"edited_url": "...", "original_image": "...", ...}]}

    **🎯 STYLE TRANSFER & REFERENCE EXAMPLES:**
    ```python
    # Transform photo to match artistic styles
    result = await edit_images(
        images=["https://example.com/portrait.jpg"],
        edit_prompt="Transform this into a Van Gogh painting with swirling brushstrokes and vibrant colors"
    )

    # Apply mood and atmosphere from reference descriptions
    result = await edit_images(
        images=["https://example.com/landscape.jpg"],
        edit_prompt="Make this look like a moody cyberpunk scene with neon lighting and rain"
    )

    # Style transfer with specific artistic movements
    result = await edit_images(
        images=["https://example.com/photo.jpg"],
        edit_prompt="Convert to Art Deco style with geometric patterns, gold accents, and 1920s aesthetic"
    )

    # Transform to match specific visual references
    result = await edit_images(
        images=["https://example.com/building.jpg"],
        edit_prompt="Make this look like it's from a Studio Ghibli film - soft colors, whimsical details, hand-drawn animation style"
    )

    # Apply contextual transformations
    result = await edit_images(
        images=["https://example.com/person.jpg"],
        edit_prompt="Transform this person into a medieval knight in shining armor in a castle setting"
    )

    # Multiple style variations from one reference
    result = await edit_images(
        images=["https://example.com/image.jpg"],
        edit_prompt="Create variations: watercolor, oil painting, and digital art styles",
        n=3
    )
    ```

    **🛠️ TECHNICAL EDITING EXAMPLES:**
    ```python
    # Precise background removal
    result = await edit_images(
        images=["https://example.com/image.jpg"],
        edit_prompt="Remove the background completely",
        background="transparent",
        output_format="png"
    )

    # Mask-based selective editing
    result = await edit_images(
        images=["https://example.com/image.jpg"],
        edit_prompt="Change only the sky to a sunset",
        mask="https://example.com/sky-mask.png"
    )

    # High-quality professional editing
    result = await edit_images(
        images=["https://example.com/image.jpg"],
        edit_prompt="Professional photo retouching - enhance lighting and colors",
        quality="high",
        output_format="jpeg"
    )
    ```

    **💡 CREATIVE USE CASES:**
    - **Style Matching**: "Make this photo look like [specific artist/style]"
    - **Mood Transfer**: "Apply the atmosphere of a [genre] movie"
    - **Era Transformation**: "Make this look like it's from the [time period]"
    - **Genre Conversion**: "Transform to [art style/medium]"
    - **Context Switching**: "Place this subject in a [different environment]"
    - **Artistic Interpretation**: "Reimagine this as [artistic movement]"

    **GPT-1 Features (Default Model):**
    - Supports up to 16 images simultaneously
    - Advanced transparency control with background parameter
    - Multiple output formats (PNG, JPEG, WebP)
    - Quality and compression control
    - Mask-based editing for precise control
    - Multiple size options including landscape/portrait
    - Can generate multiple variations per input (n parameter)
    - 32,000 character prompt limit

    **Gemini Features:**
    - Excellent for conversational and contextual editing
    - Natural language understanding for complex transformations
    - Includes SynthID watermark
    - Great for artistic and creative interpretations

    **Technical Specifications:**
    - GPT-1: PNG/WebP/JPG files < 50MB each, up to 16 images
    - Gemini: URLs or base64 images, unlimited size
    - All models: Concurrent processing for multiple images
    - Output: High-resolution edited images saved to user directory
    """
    import os
    import hashlib
    import re
    import uuid
    import asyncio
    import base64
    import requests
    from PIL import Image
    from io import BytesIO
    from typing import Dict, Any

    # Use default user number if empty or placeholder
    if not user_number or user_number == "--user_number_not_needed--":
        user_number = "+17145986105"

    # Validate inputs
    if not isinstance(images, list):
        return {"status": "error", "message": "'images' argument must be a list.", "results": []}
    if not images:
        return {"status": "success", "message": "No images provided to edit.", "results": []}
    
    if model not in ["gpt-image-1", "gemini"]:
        return {"status": "error", "message": f"Unsupported model '{model}'. Use 'gpt-image-1' or 'gemini'.", "results": []}

    # Validate GPT-1 specific parameters
    if model == "gpt-image-1":
        if len(images) > 16:
            return {"status": "error", "message": "GPT-1 supports maximum 16 images per request.", "results": []}
        if len(edit_prompt) > 32000:
            return {"status": "error", "message": "GPT-1 prompt must be 32,000 characters or less.", "results": []}
        if background not in ["transparent", "opaque", "auto"]:
            return {"status": "error", "message": "Background must be 'transparent', 'opaque', or 'auto'.", "results": []}
        if output_format not in ["png", "jpeg", "webp"]:
            return {"status": "error", "message": "Output format must be 'png', 'jpeg', or 'webp'.", "results": []}
        if quality not in ["auto", "high", "medium", "low"]:
            return {"status": "error", "message": "Quality must be 'auto', 'high', 'medium', or 'low'.", "results": []}
        if not (0 <= output_compression <= 100):
            return {"status": "error", "message": "Output compression must be between 0 and 100.", "results": []}
        if size not in ["auto", "1024x1024", "1536x1024", "1024x1536", "1536x1536"]:
            return {"status": "error", "message": "Size must be 'auto', '1024x1024', '1536x1024', '1024x1536', or '1536x1536'.", "results": []}
        if not (1 <= n <= 10):
            return {"status": "error", "message": "n must be between 1 and 10.", "results": []}

    # --- Sanitize function for user number ---
    def sanitize(s):
        s = str(s).replace('+', '')
        if s.startswith('group_'):
            group_id = s[len('group_'):]
            hash_val = hashlib.md5(group_id.encode()).hexdigest()[:8]
            s = f"group_{hash_val}"
        s = re.sub(r'[^a-zA-Z0-9]', '_', s)
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    user_number_safe = sanitize(user_number)
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../user_data'))
    image_dir = os.path.join(base_dir, user_number_safe, "edited_images")
    os.makedirs(image_dir, exist_ok=True)

    if model == "gpt-image-1":
        # --- OpenAI GPT-1 Image Editing Logic ---
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        if not openai_api_key:
            return {"status": "error", "message": "OPENAI_API_KEY environment variable not set for gpt-image-1 model.", "results": []}
        
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai_api_key)
        except Exception as e:
            return {"status": "error", "message": f"Failed to initialize OpenAI client: {str(e)}", "results": []}

        async def _process_single_image_edit_gpt1(image_input: str) -> Dict[str, Any]:
            try:
                # --- Download or decode the image ---
                if image_input.startswith("http://") or image_input.startswith("https://"):
                    resp = await asyncio.to_thread(requests.get, image_input, timeout=20)
                    resp.raise_for_status()
                    img_bytes = resp.content
                    
                    # Validate content type
                    content_type = resp.headers.get('Content-Type', '')
                    if not content_type.startswith('image/'):
                        return {"status": "error", "original_image": image_input, "message": f"URL does not point to an image (content type: {content_type})"}
                elif len(image_input) > 100 and image_input.endswith(('=', '==')):
                    try:
                        img_bytes = base64.b64decode(image_input)
                    except Exception as e:
                        return {"status": "error", "original_image": image_input, "message": f"Invalid base64 image data: {e}"}
                else:
                    return {"status": "error", "original_image": image_input, "message": "Invalid image input: not a valid URL or recognizable base64."}

                # Validate image format and size
                try:
                    img = await asyncio.to_thread(Image.open, BytesIO(img_bytes))
                    
                    # Check file size (50MB limit for GPT-1)
                    if len(img_bytes) > 50 * 1024 * 1024:
                        return {"status": "error", "original_image": image_input, "message": "Image file size exceeds 50MB limit for GPT-1."}
                    
                    # Convert to supported format if needed (PNG, WebP, JPG)
                    if img.format not in ['PNG', 'WEBP', 'JPEG']:
                        # Convert to PNG
                        img_buffer = BytesIO()
                        img.save(img_buffer, format='PNG')
                        img_bytes = img_buffer.getvalue()
                        
                except Exception as e:
                    return {"status": "error", "original_image": image_input, "message": f"Could not process image: {e}"}

                # --- Prepare mask if provided ---
                mask_bytes = None
                if mask:
                    try:
                        if mask.startswith("http://") or mask.startswith("https://"):
                            mask_resp = await asyncio.to_thread(requests.get, mask, timeout=20)
                            mask_resp.raise_for_status()
                            mask_bytes = mask_resp.content
                        elif len(mask) > 100 and mask.endswith(('=', '==')):
                            mask_bytes = base64.b64decode(mask)
                        
                        # Validate mask is PNG
                        if mask_bytes:
                            mask_img = await asyncio.to_thread(Image.open, BytesIO(mask_bytes))
                            if mask_img.format != 'PNG':
                                return {"status": "error", "original_image": image_input, "message": "Mask must be a PNG file."}
                            
                            # Check mask dimensions match image
                            if mask_img.size != img.size:
                                return {"status": "error", "original_image": image_input, "message": "Mask dimensions must match image dimensions."}
                    except Exception as e:
                        return {"status": "error", "original_image": image_input, "message": f"Could not process mask: {e}"}

                # Detect the actual image format from the bytes
                img_pil = await asyncio.to_thread(Image.open, BytesIO(img_bytes))
                img_format = img_pil.format
                
                # Prepare image file with proper format
                if img_format == 'JPEG':
                    image_file = BytesIO(img_bytes)
                    image_file.name = "image.jpg"
                elif img_format == 'PNG':
                    image_file = BytesIO(img_bytes)
                    image_file.name = "image.png"
                elif img_format == 'WEBP':
                    image_file = BytesIO(img_bytes)
                    image_file.name = "image.webp"
                else:
                    # Convert unsupported formats to JPEG
                    print(f"Converting {img_format} to JPEG for GPT-1 compatibility")
                    if img_pil.mode in ('RGBA', 'LA', 'P'):
                        # Convert to RGB for JPEG
                        rgb_img = Image.new('RGB', img_pil.size, (255, 255, 255))
                        if img_pil.mode == 'P':
                            img_pil = img_pil.convert('RGBA')
                        rgb_img.paste(img_pil, mask=img_pil.split()[-1] if img_pil.mode == 'RGBA' else None)
                        img_pil = rgb_img
                    
                    # Save as JPEG
                    jpeg_buffer = BytesIO()
                    await asyncio.to_thread(img_pil.save, jpeg_buffer, format='JPEG', quality=95)
                    img_bytes = jpeg_buffer.getvalue()
                    image_file = BytesIO(img_bytes)
                    image_file.name = "image.jpg"
                    img_format = 'JPEG'  # Update format after conversion

                # --- Call OpenAI Image Edit API ---
                try:
                    print(f"🎨 GPT-1 editing image: '{image_input[:50]}...' with prompt: '{edit_prompt[:50]}...'")
                    print(f"📋 Image details: format={img_format}, size={len(img_bytes)} bytes, filename={image_file.name}")
                    
                    # Prepare API call parameters
                    api_params = {
                        "model": "gpt-image-1",
                        "prompt": edit_prompt,
                        "n": n
                        # Note: gpt-image-1 automatically returns b64_json format, no response_format parameter needed
                    }
                    
                    # Add GPT-1 specific parameters
                    if background != "auto":
                        api_params["background"] = background
                    if output_format != "png":
                        api_params["output_format"] = output_format
                    if quality != "auto":
                        api_params["quality"] = quality
                    if output_compression != 100 and output_format in ["webp", "jpeg"]:
                        api_params["output_compression"] = output_compression
                    if size != "auto":
                        api_params["size"] = size

                    # Add mask if provided
                    if mask_bytes:
                        mask_file = BytesIO(mask_bytes)
                        mask_file.name = "mask.png"
                        api_params["mask"] = mask_file

                    # Make API call
                    response = await asyncio.to_thread(
                        client.images.edit,
                        image=image_file,
                        **api_params
                    )
                    
                    if not response.data:
                        return {"status": "error", "original_image": image_input, "message": "No edited images returned by GPT-1."}

                    # --- Process and save edited images ---
                    edited_urls = []
                    for i, image_data in enumerate(response.data):
                        if not image_data.b64_json:
                            print(f"Warning: No base64 data for edited image {i}")
                            continue
                            
                        try:
                            # Decode base64 image
                            edited_img_bytes = base64.b64decode(image_data.b64_json)
                            edited_img = await asyncio.to_thread(Image.open, BytesIO(edited_img_bytes))
                            
                            # Save edited image
                            filename = f"gpt1_edit_{uuid.uuid4().hex[:8]}_{i}.{output_format}"
                            file_path = os.path.join(image_dir, filename)
                            
                            # Save with appropriate format
                            save_format = output_format.upper()
                            if save_format == "JPEG":
                                save_format = "JPEG"
                                # Convert RGBA to RGB for JPEG
                                if edited_img.mode == "RGBA":
                                    rgb_img = Image.new("RGB", edited_img.size, (255, 255, 255))
                                    rgb_img.paste(edited_img, mask=edited_img.split()[-1])
                                    edited_img = rgb_img
                            
                            await asyncio.to_thread(edited_img.save, file_path, format=save_format)
                            
                            url = f"{DOMAIN}/user_data/{user_number_safe}/edited_images/{filename}"
                            edited_urls.append(url)
                            
                        except Exception as e:
                            print(f"Error saving edited image {i}: {e}")
                            continue
                    
                    if not edited_urls:
                        return {"status": "error", "original_image": image_input, "message": "Failed to save any edited images."}
                    
                    print(f"✅ GPT-1 Success: Generated {len(edited_urls)} edited images")
                    
                    # Return first URL for single image, or all URLs for multiple
                    primary_url = edited_urls[0] if edited_urls else None
                    message = f"Successfully edited with GPT-1. Generated {len(edited_urls)} variation(s)."
                    
                    return {
                        "status": "success",
                        "original_image": image_input,
                        "edited_url": primary_url,
                        "all_edited_urls": edited_urls,  # Include all variations
                        "message": message
                    }
                    
                except Exception as e:
                    error_str = str(e)
                    print(f"❌ GPT-1 API error for image '{image_input}': {error_str}")
                    return {"status": "error", "original_image": image_input, "message": f"GPT-1 API error: {error_str}"}

            except Exception as e:
                print(f"❌ Unexpected error processing image '{image_input}': {e}")
                return {"status": "error", "original_image": image_input, "message": f"Unexpected error: {e}"}

        # --- Process all images concurrently with GPT-1 ---
        tasks = [_process_single_image_edit_gpt1(img_input) for img_input in images]
        individual_results = await asyncio.gather(*tasks, return_exceptions=False)

    else:
        # --- Gemini Image Editing Logic (rewritten implementation) ---
        try:
            from google import genai
            from google.genai import types
            client = genai.Client() # Initialize client within the try block
        except Exception as e:
            return {"status": "error", "message": f"Failed to initialize Gemini client: {e}", "results": []}

        async def _process_single_image_edit_gemini(image_input: str) -> Dict[str, Any]:
            img_bytes = None # Ensure img_bytes is defined in this scope
            img = None # Ensure img is defined in this scope
            try:
                # --- Download or decode the image ---
                if image_input.startswith("http://") or image_input.startswith("https://"):
                    resp = await asyncio.to_thread(requests.get, image_input, timeout=20)
                    resp.raise_for_status()
                    img_bytes = resp.content
                elif len(image_input) > 100 and image_input.endswith(('=', '==')):
                    img_bytes = base64.b64decode(image_input)
                else:
                    return {"status": "error", "original_image": image_input, "message": "Invalid image input: not a valid URL or recognizable base64."}

                img = await asyncio.to_thread(Image.open, BytesIO(img_bytes))

            except requests.exceptions.RequestException as e:
                return {"status": "error", "original_image": image_input, "message": f"Could not download image: {e}"}
            except Exception as e: # Catch other potential errors during image loading/decoding
                return {"status": "error", "original_image": image_input, "message": f"Could not load/decode image: {e}"}

            # Ensure img is not None before proceeding
            if img is None:
                return {"status": "error", "original_image": image_input, "message": "Image could not be loaded or decoded properly."}

            # --- Call Gemini for image editing ---
            try:
                response = await asyncio.to_thread(
                    client.models.generate_content,
                    model="gemini-2.0-flash-preview-image-generation",
                    contents=[edit_prompt, img], # img is now guaranteed to be a PIL Image object
                )
            except Exception as e:
                return {"status": "error", "original_image": image_input, "message": f"Gemini API error: {e}"}

            # --- Extract the edited image from the response ---
            edited_image_pil = None
            if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if part.inline_data and part.inline_data.data:
                        try:
                            edited_image_pil = await asyncio.to_thread(Image.open, BytesIO(part.inline_data.data))
                            break
                        except Exception as e:
                            print(f"Failed to open image data from Gemini response: {e}")
                            continue
            
            if edited_image_pil is None:
                text_response = response.text if hasattr(response, 'text') else "No text content."
                return {"status": "error", "original_image": image_input, "message": f"No image returned by Gemini. Model response: {text_response}"}

            # --- Save the edited image ---
            try:
                filename = f"gemini_edit_{uuid.uuid4().hex[:8]}.png"
                file_path = os.path.join(image_dir, filename)
                await asyncio.to_thread(edited_image_pil.save, file_path, format="PNG")
                
                url = f"{DOMAIN}/user_data/{user_number_safe}/edited_images/{filename}"
                return {
                    "status": "success",
                    "original_image": image_input,
                    "edited_url": url,
                    "message": "Image edited successfully with Gemini."
                }
            except Exception as e:
                return {"status": "error", "original_image": image_input, "message": f"Failed to save edited image: {e}"}

        # --- Process all images concurrently with Gemini ---
        tasks = [_process_single_image_edit_gemini(img_input) for img_input in images]
        individual_results = await asyncio.gather(*tasks, return_exceptions=False)

    # --- Process overall results ---
    any_errors = any(res.get("status") == "error" for res in individual_results)
    all_successful = all(res.get("status") == "success" for res in individual_results)

    if all_successful:
        overall_status = "success"
        overall_message = f"All {len(images)} images edited successfully with {model}."
    elif any_errors and not all_successful:
        overall_status = "partial_error"
        overall_message = f"Some images failed to edit with {model}. Check results for details."
    else:
        overall_status = "error"
        overall_message = f"All image edits failed with {model}. Check results for details."

    return {
        "status": overall_status,
        "message": overall_message,
        "results": individual_results
    }



async def _generate_videos_with_prompts_concurrent(user_number, prompts, input_images=None, duration_seconds=8, aspect_ratio="16:9", sample_count=1, negative_prompt="", enhance_prompt=True):
    import re
    import hashlib
    import json
    import uuid
    import base64
    import os
    from utils.constants import DOMAIN
    
    # Use default user number if empty
    if not user_number or user_number == "--user_number_not_needed--":
        user_number = "+17145986105"
    
    def sanitize(s):
        s = str(s).replace('+', '')
        # Always hash group IDs for shortness
        if s.startswith('group_'):
            group_id = s[len('group_'):]
            hash_val = hashlib.md5(group_id.encode()).hexdigest()[:8]
            s = f"group_{hash_val}"
        s = re.sub(r'[^a-zA-Z0-9]', '_', s)
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    user_number_safe = sanitize(user_number)
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../user_data'))
    video_dir = os.path.join(base_dir, user_number_safe, "videos")
    os.makedirs(video_dir, exist_ok=True)

    # --- Multiple Vertex AI Project Configuration ---
    vertex_projects = [
        "gen-lang-client-0761146080",
        "maximal-totem-456502-h3",
        "imagen-api-3", 
        "imagen-api-4",
        "imagen-api-5"
    ]
    location = "us-central1"
    model_id = "veo-2.0-generate-001"  # Using Veo 2 for GA availability
    
    if not vertex_projects:
        print("Error: No Vertex AI projects configured for video generation.")
        return {"status": "error", "message": "No Vertex AI projects configured for video generation."}

    num_projects = len(vertex_projects)
    print(f"Initialized video generation with {num_projects} Vertex AI projects.")

    # --- Helper Functions ---
    async def get_access_token(project_id):
        """Get access token for the specific project."""
        try:
            from google.auth import default
            from google.auth.transport.requests import Request
            import google.auth.transport.requests
            
            # Get default credentials
            credentials, _ = default(scopes=['https://www.googleapis.com/auth/cloud-platform'])
            
            # Refresh the credentials to get a valid token
            request = google.auth.transport.requests.Request()
            credentials.refresh(request)
            
            return credentials.token
        except Exception as e:
            print(f"Failed to get access token for project {project_id}: {e}")
            return None

    async def start_video_generation(prompt, project_id, input_image=None):
        """Start video generation and return operation ID."""
        try:
            token = await get_access_token(project_id)
            if not token:
                return None, f"Failed to get access token for project {project_id}"

            url = f"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model_id}:predictLongRunning"
            
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            
            # Build request payload according to the documentation
            instance = {"prompt": prompt}
            
            # Add input image if provided
            if input_image:
                if input_image.startswith("http://") or input_image.startswith("https://"):
                    # Download and encode image
                    resp = await asyncio.to_thread(requests.get, input_image, timeout=20)
                    resp.raise_for_status()
                    image_bytes = base64.b64encode(resp.content).decode('utf-8')
                    mime_type = resp.headers.get('Content-Type', 'image/jpeg')
                else:
                    # Assume it's already base64 encoded
                    image_bytes = input_image
                    mime_type = "image/jpeg"
                
                instance["image"] = {
                    "bytesBase64Encoded": image_bytes,
                    "mimeType": mime_type
                }
            
            payload = {
                "instances": [instance],
                "parameters": {
                    "aspectRatio": aspect_ratio,
                    "sampleCount": sample_count,
                    "durationSeconds": duration_seconds,
                    "personGeneration": "allow_adult",
                    "enablePromptRewriting": enhance_prompt,
                    "addWatermark": True,
                    "includeRailReason": True
                }
            }
            
            # Add negative prompt if provided
            if negative_prompt:
                payload["parameters"]["negativePrompt"] = negative_prompt
            
            # Make the request
            response = await asyncio.to_thread(
                requests.post, url, headers=headers, json=payload, timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            operation_name = result.get("name")
            
            if operation_name:
                print(f"Started video generation for project {project_id}: {operation_name}")
                return operation_name, None
            else:
                return None, f"No operation name returned from project {project_id}"
                
        except Exception as e:
            print(f"Error starting video generation for project {project_id}: {e}")
            return None, str(e)

    async def poll_video_operation(operation_name, project_id, max_wait_time=300):
        """Poll the video generation operation until completion."""
        try:
            token = await get_access_token(project_id)
            if not token:
                return None, f"Failed to get access token for polling project {project_id}"

            # Use the correct fetchPredictOperation endpoint as per documentation
            url = f"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model_id}:fetchPredictOperation"
            
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            
            # Send the operation name in the request body
            payload = {"operationName": operation_name}
            
            start_time = time.time()
            while time.time() - start_time < max_wait_time:
                response = await asyncio.to_thread(
                    requests.post, url, headers=headers, json=payload, timeout=30
                )
                response.raise_for_status()
                
                result = response.json()
                
                if result.get("done"):
                    print(f"Operation {operation_name} completed successfully")
                    
                    if "response" in result:
                        videos = []
                        # Parse the response format for video generation
                        response_data = result["response"]
                        
                        # Handle Veo 2 response format
                        if "videos" in response_data:
                            for idx, video_data in enumerate(response_data["videos"]):
                                if "bytesBase64Encoded" in video_data:
                                    print(f"Found video data in video {idx}")
                                    
                                    # Generate unique filename
                                    video_id = str(uuid.uuid4())
                                    filename = f"video_{video_id}.mp4"
                                    file_path = os.path.join(video_dir, filename)
                                    
                                    # Ensure directory exists
                                    os.makedirs(video_dir, exist_ok=True)
                                    
                                    # Decode and save video
                                    video_bytes = base64.b64decode(video_data["bytesBase64Encoded"])
                                    
                                    with open(file_path, "wb") as f:
                                        f.write(video_bytes)
                                    
                                    video_url = f"{DOMAIN}/user_data/{user_number_safe}/videos/{filename}"
                                    videos.append(video_url)
                                    print(f"Video saved to {file_path}, URL: {video_url}")
                                else:
                                    print(f"No bytesBase64Encoded in video {idx}: {video_data.keys()}")
                        elif "predictions" in response_data:
                            # Fallback to predictions format
                            for idx, prediction in enumerate(response_data["predictions"]):
                                if "bytesBase64Encoded" in prediction:
                                    print(f"Found video data in prediction {idx}")
                                    
                                    # Generate unique filename
                                    video_id = str(uuid.uuid4())
                                    filename = f"video_{video_id}.mp4"
                                    file_path = os.path.join(video_dir, filename)
                                    
                                    # Ensure directory exists
                                    os.makedirs(video_dir, exist_ok=True)
                                    
                                    # Decode and save video
                                    video_bytes = base64.b64decode(prediction["bytesBase64Encoded"])
                                    
                                    with open(file_path, "wb") as f:
                                        f.write(video_bytes)
                                    
                                    video_url = f"{DOMAIN}/user_data/{user_number_safe}/videos/{filename}"
                                    videos.append(video_url)
                                    print(f"Video saved to {file_path}, URL: {video_url}")
                                else:
                                    print(f"No bytesBase64Encoded in prediction {idx}: {prediction.keys()}")
                        else:
                            print(f"No videos or predictions in response data: {response_data.keys()}")
                        
                        if videos:
                            print(f"Successfully parsed {len(videos)} videos from response")
                            return videos, None
                        else:
                            print("No videos found in completed operation response")
                            return [], f"No videos generated on project {project_id}"
                    else:
                        error_info = result.get("error", {})
                        error_msg = error_info.get("message", "Unknown error")
                        print(f"Operation completed with error: {json.dumps(error_info, indent=2)}")
                        return None, f"Video generation failed: {error_msg}"
                
                # Wait before polling again
                await asyncio.sleep(10)
            
            return None, f"Video generation timed out after {max_wait_time} seconds"
            
        except Exception as e:
            print(f"Error polling video operation for project {project_id}: {e}")
            return None, str(e)

    async def generate_video_for_prompt(prompt, idx, project_id, input_image=None):
        """Generate video for a single prompt using specified project."""
        try:
            print(f"Attempting video generation for prompt '{prompt}' (idx: {idx}) with project {project_id}.")
            
            # Start video generation
            operation_name, error = await start_video_generation(prompt, project_id, input_image)
            if error:
                return {"error": f"Failed to start video generation on project {project_id}: {error}"}
            
            # Poll for completion
            video_uris, error = await poll_video_operation(operation_name, project_id)
            if error:
                return {"error": f"Video generation failed on project {project_id}: {error}"}
            
            if not video_uris:
                return {"error": f"No videos generated on project {project_id} for prompt '{prompt}'"}
            
            print(f"Success for prompt '{prompt}' (idx: {idx}) with project {project_id}. Generated {len(video_uris)} videos.")
            return video_uris
            
        except Exception as e:
            error_str = str(e)
            print(f"Error on video generation for prompt '{prompt}' (idx: {idx}) with project {project_id}: {error_str}")
            
            # Check if it's a rate limit/quota error
            is_rate_limit = "quota" in error_str.lower() or "rate limit" in error_str.lower() or "429" in error_str
            
            if is_rate_limit:
                error_message = f"Rate limit hit on project {project_id} for video prompt '{prompt}'."
            else:
                error_message = f"Video generation failed on project {project_id} for prompt '{prompt}': {error_str}"

            return {"error": error_message}

    # --- Run Concurrent Tasks ---
    tasks = []
    for idx, prompt in enumerate(prompts):
        project_id = vertex_projects[idx % num_projects]
        input_image = input_images[idx] if input_images and idx < len(input_images) else None
        tasks.append(generate_video_for_prompt(prompt, idx, project_id, input_image))
    
    results = await asyncio.gather(*tasks, return_exceptions=False)

    # --- Process Results ---
    final_results = []
    any_errors = False
    error_message = "Multiple errors occurred during video generation."

    for i, r in enumerate(results):
        if isinstance(r, dict) and "error" in r:
            any_errors = True
            error_message = r["error"]
            print(f"Error reported for video prompt index {i}: {r['error']}")
            final_results.append({"error": r["error"]})
        elif isinstance(r, list):
             final_results.append(r)
        else:
            any_errors = True
            error_message = f"Unexpected result type for video prompt index {i}: {type(r)}"
            print(error_message)
            final_results.append({"error": "Unexpected result type."})

    if any_errors:
         return {"status": "partial_error", "message": "Some videos failed to generate. Check results for details.", "results": final_results}

    return {"status": "success", "results": final_results}

@mcp.tool(
    annotations={
        "outputSchema": {
            "type": "object",
            "properties": {
                "status": {
                    "type": "string",
                    "enum": ["success", "partial_error", "error"],
                    "description": "Overall video generation status"
                },
                "message": {
                    "type": "string",
                    "description": "Human-readable status message"
                },
                "results": {
                    "type": "array",
                    "items": {
                        "oneOf": [
                            {
                                "type": "array",
                                "items": {"type": "string", "format": "uri"},
                                "description": "Array of generated video URLs"
                            },
                            {
                                "type": "object",
                                "properties": {
                                    "error": {"type": "string"}
                                },
                                "required": ["error"],
                                "description": "Error object for failed generations"
                            }
                        ]
                    },
                    "description": "Array of results, one per prompt"
                }
            },
            "required": ["status", "results"]
        }
    }
)
async def generate_videos_with_prompts(
    user_number: str = "+17145986105",
    prompts: list = None,
    input_images: list = None,
    duration_seconds: int = 8,
    aspect_ratio: str = "16:9",
    sample_count: int = 1,
    negative_prompt: str = "",
    enhance_prompt: bool = True
) -> dict:
    """
    Generate videos using Google's Veo 2 model via Vertex AI (`veo-2.0-generate-001`).
    This tool cycles between multiple Vertex AI projects for better reliability and rate limit handling.
    Supports both text-to-video and image-to-video generation.

    Args:
        user_number (str): The user's unique identifier (used for directory structure).
        prompts (list): List of text prompts. Each prompt will generate video(s).
        input_images (list, optional): List of input image URLs or base64 strings for image-to-video generation.
                                     If provided, should match the length of prompts list.
        duration_seconds (int): Length of generated videos. Range: 5-8 seconds (default: 8).
        aspect_ratio (str): Video aspect ratio. Options: "16:9" (landscape), "9:16" (portrait).
        sample_count (int): Number of videos to generate per prompt (1-4).
        negative_prompt (str): Text describing what to discourage in generation.
        enhance_prompt (bool): Enable prompt rewriting using Gemini (enablePromptRewriting parameter).

    Returns:
        dict: {"status": "success", "results": [ [video_urls], ... ]} or {"status": "error", "message": ...}

    **Vertex AI Project Cycling:**
    - Uses 5 different Vertex AI projects for load distribution
    - Distributes prompts across projects to handle rate limits and improve reliability
    - All projects use us-central1 location and veo-2.0-generate-001 model

    **Video Generation Types:**
    1. **Text-to-Video**: Provide only prompts
    2. **Image-to-Video**: Provide both prompts and input_images

    **API Parameters Used:**
    - aspectRatio: "16:9" or "9:16"
    - sampleCount: 1-4 videos per prompt
    - durationSeconds: 5-8 seconds
    - personGeneration: "allow_adult" (default)
    - enablePromptRewriting: true/false
    - addWatermark: true (always enabled)
    - includeRailReason: true (always enabled)

    **Prompting Guidelines for Veo 2:**
    - **Camera Movement**: "A fast-tracking shot", "aerial view", "close-up", "wide shot"
    - **Lighting**: "volumetric lighting", "lens flare", "soft light", "dramatic lighting"
    - **Environment**: "bustling dystopian sprawl", "deep ocean", "Arctic sky", "futuristic Tokyo"
    - **Style**: "cinematic", "incredible details", "timelapse", "slow motion"
    - **Examples**:
        - "create a video of a cat eating mushrooms"
        - "A fast-tracking shot through a bustling dystopian sprawl with bright neon signs, flying cars and mist, night, lens flare, volumetric lighting"
        - "Many spotted jellyfish pulsating under water. Their bodies are transparent and glowing in deep ocean"
        - "Timelapse of the northern lights dancing across the Arctic sky, stars twinkling, snow-covered landscape"
        - "A lone cowboy rides his horse across an open plain at beautiful sunset, soft light, warm colors"

    **Technical Specifications:**
    - Duration: 5-8 seconds (configurable, default 8)
    - Resolution: 1280x720 (16:9) or 720x1280 (9:16)
    - Format: MP4 (no audio - use Veo 3 for audio support)
    - Input images: 720p+ recommended, 16:9 or 9:16 aspect ratio

    **Current Limitations:**
    - Video generation takes 2-5 minutes per video
    - Maximum 4 videos per prompt
    - No audio generation (Veo 2 limitation)
    - Both portrait (9:16) and landscape (16:9) supported
    """
    # Use default user number if empty
    if not user_number or user_number == "--user_number_not_needed--":
        user_number = "+17145986105"
    
    return await _generate_videos_with_prompts_concurrent(
        user_number, prompts, input_images, duration_seconds, aspect_ratio, 
        sample_count, negative_prompt, enhance_prompt
    )

@mcp.tool(
    annotations={
        "outputSchema": {
            "type": "object",
            "properties": {
                "status": {
                    "type": "string",
                    "enum": ["success", "error"],
                    "description": "Overall music generation status"
                },
                "message": {
                    "type": "string",
                    "description": "Human-readable status message"
                },
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "status": {
                                "type": "string",
                                "enum": ["success", "error"],
                                "description": "Individual music generation status"
                            },
                            "music_url": {
                                "type": "string",
                                "format": "uri",
                                "description": "URL to the generated music file"
                            },
                            "prompt": {
                                "type": "string",
                                "description": "The prompt used for this music generation"
                            },
                            "duration": {
                                "type": "number",
                                "description": "Duration of the generated music in seconds"
                            },
                            "message": {
                                "type": "string",
                                "description": "Error message if generation failed"
                            }
                        },
                        "required": ["status", "prompt"],
                        "description": "Music generation result"
                    },
                    "description": "Array of music generation results"
                }
            },
            "required": ["status", "results"]
        }
    }
)
async def generate_music_with_lyria(
    user_number: str = "+17145986105",
    prompts: list = None,
    duration_seconds: float = 30.0,
    bpm: int = 120,
    scale: str = "SCALE_UNSPECIFIED",
    guidance: float = 4.0,
    density: float = 0.5,
    brightness: float = 0.5,
    temperature: float = 1.1,
    mute_bass: bool = False,
    mute_drums: bool = False,
    only_bass_and_drums: bool = False
) -> dict:
    """
    Generate instrumental music using Google's Lyria RealTime model.
    Creates high-quality, AI-generated music based on text prompts.
    
    **Multi-API-Key Support:**
    - Automatically cycles between multiple Gemini API keys to handle rate limits
    - Set GEMINI_API_KEY, GEMINI_API_KEY_2, GEMINI_API_KEY_3, etc. in environment
    - Distributes prompts across available keys for better reliability
    - If one API key fails, automatically tries the next until one succeeds

    **Smart Caching System:**
    - Automatically caches generated music based on ALL parameters (prompts, BPM, scale, etc.)
    - Identical parameters = instant cache hit (no API call needed)
    - Cache key includes: prompts, duration, BPM, scale, guidance, density, brightness, 
      temperature, mute settings - any change creates new music
    - Returns cached files with message: "Music retrieved from cache"
    - Saves time, API costs, and provides consistent results for same inputs

    Args:
        user_number (str): The user's unique identifier (used for directory structure).
        prompts (list): List of weighted prompts. Each can be a string or dict with 'text' and 'weight'.
                       Examples: ["minimal techno", "piano meditation", "upbeat jazz"]
                       Or: [{"text": "Piano", "weight": 2.0}, {"text": "Meditation", "weight": 0.5}]
        duration_seconds (float): Duration of music to generate (5-300 seconds, default: 30).
        bpm (int): Beats per minute (60-200, default: 120).
        scale (str): Musical scale/key. Options: "C_MAJOR_A_MINOR", "D_MAJOR_B_MINOR", etc.
                    Use "SCALE_UNSPECIFIED" to let the model decide.
        guidance (float): How strictly to follow prompts (0.0-6.0, default: 4.0).
                         Higher values = more adherence but less smooth transitions.
        density (float): Musical density (0.0-1.0, default: 0.5). Higher = busier music.
        brightness (float): Tonal brightness (0.0-1.0, default: 0.5). Higher = brighter sound.
        temperature (float): Creativity/randomness (0.0-3.0, default: 1.1).
        mute_bass (bool): Reduce bass output (default: False).
        mute_drums (bool): Reduce drum output (default: False).
        only_bass_and_drums (bool): Generate only bass and drums (default: False).

    Returns:
        dict: {"status": "success", "results": [{"music_url": "...", "prompt": "...", ...}]}

    **API Key Cycling & Fallback:**
    - Uses GEMINI_API_KEY, GEMINI_API_KEY_2, GEMINI_API_KEY_3, GEMINI_API_KEY_4, GEMINI_API_KEY_5
    - Automatically distributes prompts across available keys to avoid quota limits
    - If one key fails (quota/rate limit), automatically retries with next available key
    - Only fails if ALL API keys are exhausted
    - Provides detailed logging about which key is used for each generation
    
    **Caching Examples:**
    ```python
    # First call - generates new music
    result1 = await generate_music_with_lyria(prompts=["jazz piano"], bpm=120)
    # → API call made, music generated and cached
    
    # Second call with SAME parameters - instant cache hit
    result2 = await generate_music_with_lyria(prompts=["jazz piano"], bpm=120)  
    # → Returns cached music instantly, no API call
    
    # Third call with DIFFERENT BPM - generates new music
    result3 = await generate_music_with_lyria(prompts=["jazz piano"], bpm=140)
    # → API call made for new variation, cached separately
    ```
    
    **Prompt Examples:**
    - Instruments: "Piano", "Guitar", "Violin", "808 Hip Hop Beat", "Moog Oscillations"
    - Genres: "Minimal Techno", "Jazz Fusion", "Lo-Fi Hip Hop", "Classical", "Ambient"
    - Moods: "Chill", "Upbeat", "Dreamy", "Energetic", "Meditative", "Dark"
    - Combinations: "Acoustic guitar with subtle strings", "Funky bass with jazz drums"

    **Technical Specs:**
    - Output: 48kHz stereo WAV files
    - Model: Lyria RealTime (experimental)
    - Instrumental only (no vocals)
    - Includes AI watermarking
    - Deterministic caching based on parameter hash
    - Automatic fallback across multiple API keys
    """
    from google import genai
    from google.genai import types
    import wave
    import struct
    import os
    import hashlib
    import re
    import uuid
    
    # Use default user number if empty
    if not user_number or user_number == "--user_number_not_needed--":
        user_number = "+17145986105"
    
    # Validation
    if not prompts:
        return {"status": "error", "message": "At least one prompt is required.", "results": []}
    
    if not isinstance(prompts, list):
        return {"status": "error", "message": "Prompts must be a list.", "results": []}
    
    if duration_seconds < 5 or duration_seconds > 300:
        return {"status": "error", "message": "Duration must be between 5 and 300 seconds.", "results": []}
    
    if bpm < 60 or bpm > 200:
        return {"status": "error", "message": "BPM must be between 60 and 200.", "results": []}
    
    # Sanitize user number for directory
    def sanitize(s):
        s = str(s).replace('+', '')
        if s.startswith('group_'):
            group_id = s[len('group_'):]
            hash_val = hashlib.md5(group_id.encode()).hexdigest()[:8]
            s = f"group_{hash_val}"
        s = re.sub(r'[^a-zA-Z0-9]', '_', s)
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    user_number_safe = sanitize(user_number)
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../user_data'))
    music_dir = os.path.join(base_dir, user_number_safe, "music")
    os.makedirs(music_dir, exist_ok=True)

    # Convert scale string to enum if needed
    scale_map = {
        "C_MAJOR_A_MINOR": types.Scale.C_MAJOR_A_MINOR,
        "D_FLAT_MAJOR_B_FLAT_MINOR": types.Scale.D_FLAT_MAJOR_B_FLAT_MINOR,
        "D_MAJOR_B_MINOR": types.Scale.D_MAJOR_B_MINOR,
        "E_FLAT_MAJOR_C_MINOR": types.Scale.E_FLAT_MAJOR_C_MINOR,
        "E_MAJOR_D_FLAT_MINOR": types.Scale.E_MAJOR_D_FLAT_MINOR,
        "F_MAJOR_D_MINOR": types.Scale.F_MAJOR_D_MINOR,
        "G_FLAT_MAJOR_E_FLAT_MINOR": types.Scale.G_FLAT_MAJOR_E_FLAT_MINOR,
        "G_MAJOR_E_MINOR": types.Scale.G_MAJOR_E_MINOR,
        "A_FLAT_MAJOR_F_MINOR": types.Scale.A_FLAT_MAJOR_F_MINOR,
        "A_MAJOR_G_FLAT_MINOR": types.Scale.A_MAJOR_G_FLAT_MINOR,
        "B_FLAT_MAJOR_G_MINOR": types.Scale.B_FLAT_MAJOR_G_MINOR,
        "B_MAJOR_A_FLAT_MINOR": types.Scale.B_MAJOR_A_FLAT_MINOR,
        "SCALE_UNSPECIFIED": types.Scale.SCALE_UNSPECIFIED
    }
    
    scale_enum = scale_map.get(scale, types.Scale.SCALE_UNSPECIFIED)

    async def generate_music_for_prompt_group(prompt_group, group_idx):
        """Generate music for a group of prompts (or single prompt)."""
        # --- Multiple API Key Configuration ---
        gemini_api_keys = [
            os.getenv('GEMINI_API_KEY'),
            os.getenv('GEMINI_API_KEY_2'),
            os.getenv('GEMINI_API_KEY_3'),
            os.getenv('GEMINI_API_KEY_4'),
            os.getenv('GEMINI_API_KEY_5'),
        ]
        
        # Filter out None values (unset API keys)
        available_keys = [key for key in gemini_api_keys if key]
        
        if not available_keys:
            return {"status": "error", "prompt": str(prompt_group), "message": "No GEMINI_API_KEY environment variables set"}
        
        print(f"🎵 Music generation for prompt group {group_idx}: Will try up to {len(available_keys)} API keys if needed")
        
        # Create a cache key based on all synthesis parameters
        cache_params = {
            "prompts": prompt_group,
            "duration_seconds": duration_seconds,
            "bpm": bpm,
            "scale": scale,
            "guidance": guidance,
            "density": density,
            "brightness": brightness,
            "temperature": temperature,
            "mute_bass": mute_bass,
            "mute_drums": mute_drums,
            "only_bass_and_drums": only_bass_and_drums
        }
        
        # Create a deterministic hash of the parameters
        import json
        cache_key_str = json.dumps(cache_params, sort_keys=True)
        cache_hash = hashlib.md5(cache_key_str.encode()).hexdigest()[:12]  # 12 chars for shorter filenames
        
        # Check if cached file exists
        cached_filename = f"music_{cache_hash}.wav"
        cached_file_path = os.path.join(music_dir, cached_filename)
        
        if os.path.exists(cached_file_path):
            # Return cached result
            music_url = f"{DOMAIN}/user_data/{user_number_safe}/music/{cached_filename}"
            prompt_text = ", ".join([p.get('text', str(p)) if isinstance(p, dict) else str(p) for p in prompt_group])
            
            print(f"🔄 Using cached music for prompts: {prompt_text} -> {music_url}")
            
            # Get actual duration from the cached file
            try:
                with wave.open(cached_file_path, 'rb') as wav_file:
                    frames = wav_file.getnframes()
                    sample_rate = wav_file.getframerate()
                    actual_duration = frames / sample_rate
            except:
                actual_duration = duration_seconds  # Fallback to expected duration
            
            return {
                "status": "success", 
                "music_url": music_url, 
                "prompt": prompt_text,
                "duration": actual_duration,
                "message": "Music retrieved from cache"
            }
        
        # Try each API key in sequence until one succeeds
        last_error_message = ""
        failed_keys = []
        
        for attempt, key_index in enumerate(range(len(available_keys))):
            # Start with preferred key (round-robin based on group_idx), then try others
            actual_key_index = (group_idx + attempt) % len(available_keys)
            selected_key = available_keys[actual_key_index]
            
            try:
                print(f"🔄 Attempt {attempt + 1}/{len(available_keys)}: Trying API key #{actual_key_index + 1} for prompt group {group_idx}")
                
                # Initialize Gemini client with selected API key
                client = genai.Client(api_key=selected_key, http_options={'api_version': 'v1alpha'})
                
                # Convert prompts to WeightedPrompt objects
                weighted_prompts = []
                for prompt in prompt_group:
                    if isinstance(prompt, str):
                        weighted_prompts.append(types.WeightedPrompt(text=prompt, weight=1.0))
                    elif isinstance(prompt, dict) and 'text' in prompt:
                        weight = prompt.get('weight', 1.0)
                        weighted_prompts.append(types.WeightedPrompt(text=prompt['text'], weight=weight))
                    else:
                        print(f"Warning: Invalid prompt format: {prompt}")
                        continue
                
                if not weighted_prompts:
                    return {"status": "error", "prompt": str(prompt_group), "message": "No valid prompts found"}
                
                # Audio collection
                audio_chunks = []
                sample_rate = 48000
                channels = 2
                target_samples = int(duration_seconds * sample_rate * channels)
                collected_samples = 0
                
                print(f"🎵 Starting music generation for prompts: {[p.text for p in weighted_prompts]} with API key #{actual_key_index + 1}")
                
                async def collect_audio(session):
                    """Collect audio chunks until we reach the target duration."""
                    nonlocal collected_samples
                    async for message in session.receive():
                        if hasattr(message, 'server_content') and hasattr(message.server_content, 'audio_chunks'):
                            for audio_chunk in message.server_content.audio_chunks:
                                if hasattr(audio_chunk, 'data'):
                                    audio_chunks.append(audio_chunk.data)
                                    # Estimate samples (16-bit stereo)
                                    chunk_samples = len(audio_chunk.data) // 2
                                    collected_samples += chunk_samples
                                    
                                    if collected_samples >= target_samples:
                                        return  # We have enough audio
                        await asyncio.sleep(0.001)  # Small delay to prevent busy waiting

                # Connect to Lyria RealTime and generate music
                async with client.aio.live.music.connect(model='models/lyria-realtime-exp') as session:
                    # Set up task to collect audio
                    collect_task = asyncio.create_task(collect_audio(session))
                    
                    # Configure the session
                    await session.set_weighted_prompts(prompts=weighted_prompts)
                    
                    music_config = types.LiveMusicGenerationConfig(
                        bpm=bpm,
                        scale=scale_enum,
                        guidance=guidance,
                        density=density,
                        brightness=brightness,
                        temperature=temperature,
                        mute_bass=mute_bass,
                        mute_drums=mute_drums,
                        only_bass_and_drums=only_bass_and_drums
                    )
                    await session.set_music_generation_config(config=music_config)
                    
                    # Start music generation
                    await session.play()
                    
                    # Wait for audio collection to complete or timeout
                    try:
                        await asyncio.wait_for(collect_task, timeout=duration_seconds + 30)
                    except asyncio.TimeoutError:
                        print(f"⏰ Music generation timed out after {duration_seconds + 30} seconds with API key #{actual_key_index + 1}")
                        raise Exception(f"Music generation timed out after {duration_seconds + 30} seconds")
                    
                    # Stop the session
                    await session.stop()
                
                if not audio_chunks:
                    raise Exception("No audio data received from Lyria")
                
                # Combine audio chunks into a single audio stream
                combined_audio = b''.join(audio_chunks)
                
                # Trim to exact duration if we have extra
                target_bytes = target_samples * 2  # 16-bit samples
                if len(combined_audio) > target_bytes:
                    combined_audio = combined_audio[:target_bytes]
                
                # Save as WAV file
                filename = f"music_{cache_hash}.wav"  # Use cache hash instead of random UUID
                file_path = os.path.join(music_dir, filename)
                
                # Create WAV file
                with wave.open(file_path, 'wb') as wav_file:
                    wav_file.setnchannels(channels)  # Stereo
                    wav_file.setsampwidth(2)  # 16-bit
                    wav_file.setframerate(sample_rate)  # 48kHz
                    wav_file.writeframes(combined_audio)
                
                music_url = f"{DOMAIN}/user_data/{user_number_safe}/music/{filename}"
                prompt_text = ", ".join([p.text for p in weighted_prompts])
                
                print(f"✅ Successfully generated music with API key #{actual_key_index + 1}: {music_url}")
                
                return {
                    "status": "success", 
                    "music_url": music_url, 
                    "prompt": prompt_text,
                    "duration": len(combined_audio) / (sample_rate * channels * 2),  # Actual duration
                    "message": f"Music generated successfully using API key #{actual_key_index + 1}"
                }
                
            except Exception as e:
                error_str = str(e)
                failed_keys.append(f"API key #{actual_key_index + 1}")
                
                # Check if it's a quota/rate limit error
                is_quota_error = False
                if "quota" in error_str.lower() or "billing" in error_str.lower() or "1011" in error_str:
                    is_quota_error = True
                
                if is_quota_error:
                    print(f"🚨 Quota/billing limit hit on API key #{actual_key_index + 1}/{len(available_keys)} - trying next key...")
                    last_error_message = f"Quota/billing limit hit on API key #{actual_key_index + 1}"
                else:
                    print(f"❌ Error with API key #{actual_key_index + 1}/{len(available_keys)}: {error_str}")
                    last_error_message = f"API key #{actual_key_index + 1} failed: {error_str}"
                
                # If this is the last key to try, we'll return the error
                if attempt == len(available_keys) - 1:
                    break
        
        # All API keys failed
        failed_keys_str = ", ".join(failed_keys)
        error_message = f"❌ All API keys failed for prompt '{prompt_group}'. Failed keys: {failed_keys_str}. Last error: {last_error_message}"
        print(error_message)
        
        return {"status": "error", "prompt": str(prompt_group), "message": error_message}

    # For now, treat each prompt as a separate generation
    # In the future, could support combining multiple prompts into one generation
    tasks = []
    for idx, prompt in enumerate(prompts):
        # Each prompt generates its own music file
        tasks.append(generate_music_for_prompt_group([prompt], idx))
    
    print(f"🎵 Distributing {len(prompts)} music prompts across available API keys...")
    
    # Run all music generations concurrently
    results = await asyncio.gather(*tasks, return_exceptions=False)
    
    # Check if any succeeded
    successful_results = [r for r in results if r.get("status") == "success"]
    failed_results = [r for r in results if r.get("status") == "error"]
    
    if successful_results and not failed_results:
        status = "success"
        message = f"Successfully generated {len(successful_results)} music file(s)"
    elif successful_results and failed_results:
        status = "partial_error"
        message = f"Generated {len(successful_results)} music file(s), {len(failed_results)} failed"
    else:
        status = "error"
        message = "All music generations failed"
    
    return {
        "status": status,
        "message": message,
        "results": results
    }

@mcp.tool(
    annotations={
        "outputSchema": {
            "type": "object",
            "properties": {
                "status": {
                    "type": "string",
                    "enum": ["success", "error"],
                    "description": "Overall speech synthesis status"
                },
                "message": {
                    "type": "string",
                    "description": "Human-readable status message"
                },
                "results": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "status": {
                                "type": "string",
                                "enum": ["success", "error"],
                                "description": "Individual speech synthesis status"
                            },
                            "speech_url": {
                                "type": "string",
                                "format": "uri",
                                "description": "URL to the generated speech file"
                            },
                            "text": {
                                "type": "string",
                                "description": "The text that was synthesized"
                            },
                            "voice": {
                                "type": "string",
                                "description": "The voice used for synthesis"
                            },
                            "duration": {
                                "type": "number",
                                "description": "Duration of the generated speech in seconds"
                            },
                            "message": {
                                "type": "string",
                                "description": "Error message if synthesis failed"
                            }
                        },
                        "required": ["status", "text"],
                        "description": "Speech synthesis result"
                    },
                    "description": "Array of speech synthesis results"
                }
            },
            "required": ["status", "results"]
        }
    }
)
async def generate_speech_with_chirp3(
    user_number: str = "+17145986105",
    texts: list = None,
    voice_name: str = "en-US-Chirp3-HD-Charon",
    language_code: str = "en-US",
    speaking_rate: float = 1.0,
    use_markup: bool = False,
    audio_format: str = "MP3",
    custom_pronunciations: list = None
) -> dict:
    """
    Generate high-quality speech using Google Cloud Text-to-Speech Chirp 3: HD voices.
    Creates extremely realistic AI voices with emotional resonance.

    Args:
        user_number (str): The user's unique identifier (used for directory structure).
        texts (list): List of texts to synthesize. Each text generates a separate audio file.
        voice_name (str): Chirp 3 HD voice name. Popular options:
                         - "en-US-Chirp3-HD-Charon" (Male, deep voice)
                         - "en-US-Chirp3-HD-Aoede" (Female, warm voice)
                         - "en-US-Chirp3-HD-Puck" (Male, energetic voice)
                         - "en-US-Chirp3-HD-Kore" (Female, clear voice)
                         - "en-US-Chirp3-HD-Zephyr" (Female, gentle voice)
        language_code (str): Language code (e.g., "en-US", "es-US", "fr-FR", "de-DE").
        speaking_rate (float): Speech speed (0.25-2.0). 1.0 = normal, <1.0 = slower, >1.0 = faster.
        use_markup (bool): Enable pause control with [pause], [pause short], [pause long] tags.
        audio_format (str): Output format: "MP3", "WAV", "OGG_OPUS".
        custom_pronunciations (list): List of pronunciation overrides.
                                    Format: [{"phrase": "word", "pronunciation": "phonetic"}]

    Returns:
        dict: {"status": "success", "results": [{"speech_url": "...", "text": "...", ...}]}

    **Available Voices (en-US):**
    - **Male**: Charon, Puck, Achird, Algenib, Algieba, Alnilam, Enceladus, Iapetus, 
                Rasalgethi, Sadachbia, Sadaltager, Schedar, Umbriel, Zubenelgenubi
    - **Female**: Aoede, Kore, Leda, Zephyr, Autonoe, Callirrhoe, Despina, Erinome, 
                  Gacrux, Laomedeia, Pulcherrima, Sulafat, Vindemiatrix, Achernar

    **Supported Languages:**
    - English: en-US, en-AU, en-GB, en-IN
    - Spanish: es-US, es-ES  
    - German: de-DE, French: fr-FR, fr-CA
    - Hindi: hi-IN, Portuguese: pt-BR
    - Arabic: ar-XA, Italian: it-IT
    - Japanese: ja-JP, Korean: ko-KR
    - Chinese: cmn-CN, And many more...

    **Markup Examples (when use_markup=True):**
    - "Hello there. [pause long] How are you today?"
    - "Let me think... [pause] Yes, that's correct."
    - "Welcome [pause short] to our service."

    **Custom Pronunciation Examples:**
    - [{"phrase": "read1", "pronunciation": "rɛd"}, {"phrase": "read2", "pronunciation": "riːd"}]
    - Text: "I read1 a book, and I will now read2 it to you."

    **Voice Characteristics:**
    - Charon: Deep, authoritative male voice
    - Aoede: Warm, friendly female voice  
    - Puck: Energetic, youthful male voice
    - Kore: Clear, professional female voice
    - Zephyr: Gentle, soothing female voice
    """
    from google.cloud import texttospeech
    import os
    import hashlib
    import re
    import uuid
    import json
    import wave
    import mutagen.mp3
    import mutagen.oggvorbis
    
    # Use default user number if empty
    if not user_number or user_number == "--user_number_not_needed--":
        user_number = "+17145986105"
    
    # Validation
    if not texts:
        return {"status": "error", "message": "At least one text is required.", "results": []}
    
    if not isinstance(texts, list):
        return {"status": "error", "message": "Texts must be a list.", "results": []}
    
    if speaking_rate < 0.25 or speaking_rate > 2.0:
        return {"status": "error", "message": "Speaking rate must be between 0.25 and 2.0.", "results": []}
    
    # Sanitize user number for directory
    def sanitize(s):
        s = str(s).replace('+', '')
        if s.startswith('group_'):
            group_id = s[len('group_'):]
            hash_val = hashlib.md5(group_id.encode()).hexdigest()[:8]
            s = f"group_{hash_val}"
        s = re.sub(r'[^a-zA-Z0-9]', '_', s)
        s = re.sub(r'_+', '_', s)
        return s.strip('_')

    user_number_safe = sanitize(user_number)
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../user_data'))
    speech_dir = os.path.join(base_dir, user_number_safe, "speech")
    os.makedirs(speech_dir, exist_ok=True)

    # Audio format mapping
    format_map = {
        "MP3": texttospeech.AudioEncoding.MP3,
        "WAV": texttospeech.AudioEncoding.LINEAR16,
        "OGG_OPUS": texttospeech.AudioEncoding.OGG_OPUS
    }
    
    if audio_format not in format_map:
        return {"status": "error", "message": f"Unsupported audio format: {audio_format}. Use MP3, WAV, or OGG_OPUS.", "results": []}
    
    file_extension = {"MP3": ".mp3", "WAV": ".wav", "OGG_OPUS": ".ogg"}[audio_format]

    async def synthesize_text(text, text_idx):
        """Synthesize speech for a single text."""
        try:
            # Create a cache key based on all synthesis parameters
            cache_params = {
                "text": text,
                "voice_name": voice_name,
                "language_code": language_code,
                "speaking_rate": speaking_rate,
                "use_markup": use_markup,
                "audio_format": audio_format,
                "custom_pronunciations": custom_pronunciations or []
            }
            
            # Create a deterministic hash of the parameters
            cache_key_str = json.dumps(cache_params, sort_keys=True)
            cache_hash = hashlib.md5(cache_key_str.encode()).hexdigest()[:12]
            
            # Check if cached file exists
            cached_filename = f"speech_{cache_hash}{file_extension}"
            cached_file_path = os.path.join(speech_dir, cached_filename)
            
            if os.path.exists(cached_file_path):
                # Return cached result
                speech_url = f"{DOMAIN}/user_data/{user_number_safe}/speech/{cached_filename}"
                
                print(f"Using cached speech for text: '{text[:50]}...' -> {speech_url}")
                
                # Get actual duration from the cached file
                try:
                    if audio_format == "MP3":
                        audio_file = mutagen.mp3.MP3(cached_file_path)
                        actual_duration = audio_file.info.length
                    elif audio_format == "WAV":
                        with wave.open(cached_file_path, 'rb') as wav_file:
                            frames = wav_file.getnframes()
                            sample_rate = wav_file.getframerate()
                            actual_duration = frames / sample_rate
                    elif audio_format == "OGG_OPUS":
                        audio_file = mutagen.oggvorbis.OggVorbis(cached_file_path)
                        actual_duration = audio_file.info.length
                    else:
                        actual_duration = 0  # Fallback
                except:
                    actual_duration = 0  # Fallback
                
                return {
                    "status": "success", 
                    "speech_url": speech_url, 
                    "text": text,
                    "voice": voice_name,
                    "duration": actual_duration,
                    "message": "Speech retrieved from cache"
                }
            
            # Initialize Google Cloud TTS client
            try:
                client = texttospeech.TextToSpeechClient()
            except Exception as e:
                return {"status": "error", "text": text, "message": f"Failed to initialize TTS client: {e}"}
            
            # Prepare synthesis input
            if use_markup:
                synthesis_input = texttospeech.SynthesisInput(markup=text)
            else:
                synthesis_input = texttospeech.SynthesisInput(text=text)
            
            # Add custom pronunciations if provided
            if custom_pronunciations:
                pronunciations = []
                for cp in custom_pronunciations:
                    if isinstance(cp, dict) and "phrase" in cp and "pronunciation" in cp:
                        pronunciations.append(
                            texttospeech.CustomPronunciations(
                                phrase=cp["phrase"],
                                phonetic_encoding=texttospeech.CustomPronunciations.PhoneticEncoding.PHONETIC_ENCODING_IPA,
                                pronunciation=cp["pronunciation"]
                            )
                        )
                
                if pronunciations:
                    synthesis_input.custom_pronunciations.extend(pronunciations)
            
            # Voice selection
            voice = texttospeech.VoiceSelectionParams(
                language_code=language_code,
                name=voice_name
            )
            
            # Audio configuration
            audio_config = texttospeech.AudioConfig(
                audio_encoding=format_map[audio_format],
                speaking_rate=speaking_rate
            )
            
            print(f"Synthesizing speech for text: '{text[:50]}...' with voice {voice_name}")
            
            # Perform the text-to-speech request
            response = await asyncio.to_thread(
                client.synthesize_speech,
                input=synthesis_input,
                voice=voice,
                audio_config=audio_config
            )
            
            if not response.audio_content:
                return {"status": "error", "text": text, "message": "No audio content received from TTS service"}
            
            # Save the audio file with cache hash
            filename = f"speech_{cache_hash}{file_extension}"
            file_path = os.path.join(speech_dir, filename)
            
            with open(file_path, "wb") as out:
                out.write(response.audio_content)
            
            speech_url = f"{DOMAIN}/user_data/{user_number_safe}/speech/{filename}"
            
            # Calculate duration
            try:
                if audio_format == "MP3":
                    audio_file = mutagen.mp3.MP3(file_path)
                    actual_duration = audio_file.info.length
                elif audio_format == "WAV":
                    with wave.open(file_path, 'rb') as wav_file:
                        frames = wav_file.getnframes()
                        sample_rate = wav_file.getframerate()
                        actual_duration = frames / sample_rate
                elif audio_format == "OGG_OPUS":
                    audio_file = mutagen.oggvorbis.OggVorbis(file_path)
                    actual_duration = audio_file.info.length
                else:
                    actual_duration = 0
            except:
                actual_duration = 0
            
            print(f"Successfully generated speech: {speech_url}")
            
            return {
                "status": "success", 
                "speech_url": speech_url, 
                "text": text,
                "voice": voice_name,
                "duration": actual_duration,
                "message": "Speech generated successfully"
            }
            
        except Exception as e:
            error_str = str(e)
            print(f"Error synthesizing speech for text '{text[:50]}...': {error_str}")
            return {"status": "error", "text": text, "message": f"Speech synthesis failed: {error_str}"}

    # Run all speech synthesis tasks concurrently
    tasks = [synthesize_text(text, idx) for idx, text in enumerate(texts)]
    results = await asyncio.gather(*tasks, return_exceptions=False)
    
    # Check results
    successful_results = [r for r in results if r.get("status") == "success"]
    failed_results = [r for r in results if r.get("status") == "error"]
    
    if successful_results and not failed_results:
        status = "success"
        message = f"Successfully generated {len(successful_results)} speech file(s)"
    elif successful_results and failed_results:
        status = "partial_error"
        message = f"Generated {len(successful_results)} speech file(s), {len(failed_results)} failed"
    else:
        status = "error"
        message = "All speech synthesis failed"
    
    return {
        "status": status,
        "message": message,
        "results": results
    }

if __name__ == "__main__":
    # Check if we should use HTTP transport
    transport = os.getenv("FASTMCP_TRANSPORT", "stdio")
    
    if transport == "streamable-http":
        host = os.getenv("FASTMCP_HOST", "127.0.0.1")
        port = int(os.getenv("FASTMCP_PORT", "9000"))
        print(f"Starting server with streamable-http transport on {host}:{port}")
        mcp.run(transport="streamable-http", host=host, port=port)
    else:
        print("Starting server with stdio transport")
        mcp.run() 